{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "black-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "communist-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I am sleeping',\n",
    "    'I am playing',\n",
    "    'I am walking on road',\n",
    "    'I am eating'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "right-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(num_words = 100)\n",
    "\n",
    "tokenize.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenize.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "specific-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'am': 2, 'sleeping': 3, 'playing': 4, 'walking': 5, 'on': 6, 'road': 7, 'eating': 8}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mediterranean-poetry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2, 4], [1, 2, 5, 6, 7], [1, 2, 8]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenize.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-motor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conceptual-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'cool': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n",
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[5 1 3 2 4 0 0 0 0 0]\n",
      " [2 4 1 2 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is cool!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, padding = \"post\",maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-superior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "japanese-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}, {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365', 'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", 'is_sarcastic': 0}, {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697', 'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\", 'is_sarcastic': 1}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset = [json.loads(line) for line in open('Sarcasm_Headlines_Dataset.json')]\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "print(dataset[0:3])\n",
    "for item in dataset :\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cutting-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365', 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697']\n"
     ]
    }
   ],
   "source": [
    "print(urls[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "hidden-treaty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25637"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "direct-david",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "sequnces = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequnces,padding = 'post')\n",
    "print(padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "canadian-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"former versace store clerk sues over secret 'black code' for minority shoppers\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "training_labels = labels[0:training_size]\n",
    "\n",
    "testing_sentences = sentences[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "individual-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "composed-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[328, 1, 799, 3405, 2404, 47, 389, 2214, 1, 6, 2614, 8863]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fitted-effort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,  328,    1,  799, 3405, 2404,\n",
       "         47,  389, 2214,    1,    6, 2614, 8863])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cross-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    " \n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ceramic-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid')\n",
    "])\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "formed-antenna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "unexpected-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_3_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (32, 40).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_3_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (32, 40).\n",
      "  2/625 [..............................] - ETA: 3:48 - loss: 0.6904 - accuracy: 0.6719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.7254s). Check your callbacks.\n",
      "615/625 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.6808WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_3_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 38).\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.5799 - accuracy: 0.6830 - val_loss: 0.3965 - val_accuracy: 0.8314\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3168 - accuracy: 0.8720 - val_loss: 0.3434 - val_accuracy: 0.8541\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2378 - accuracy: 0.9053 - val_loss: 0.3474 - val_accuracy: 0.8495\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1930 - accuracy: 0.9270 - val_loss: 0.3607 - val_accuracy: 0.8541\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1621 - accuracy: 0.9398 - val_loss: 0.3870 - val_accuracy: 0.8518\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1370 - accuracy: 0.9500 - val_loss: 0.4185 - val_accuracy: 0.8480\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1176 - accuracy: 0.9597 - val_loss: 0.4567 - val_accuracy: 0.8433\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1022 - accuracy: 0.9660 - val_loss: 0.4945 - val_accuracy: 0.8420\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0881 - accuracy: 0.9707 - val_loss: 0.5421 - val_accuracy: 0.8408\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0777 - accuracy: 0.9747 - val_loss: 0.5892 - val_accuracy: 0.8348\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0668 - accuracy: 0.9794 - val_loss: 0.6346 - val_accuracy: 0.8332\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9817 - val_loss: 0.6793 - val_accuracy: 0.8298\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0520 - accuracy: 0.9847 - val_loss: 0.7276 - val_accuracy: 0.8272\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0467 - accuracy: 0.9865 - val_loss: 0.7782 - val_accuracy: 0.8211\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0402 - accuracy: 0.9891 - val_loss: 0.8339 - val_accuracy: 0.8195\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0379 - accuracy: 0.9887 - val_loss: 0.8867 - val_accuracy: 0.8193\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0327 - accuracy: 0.9902 - val_loss: 0.9449 - val_accuracy: 0.8165\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 1.0085 - val_accuracy: 0.8135\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 1.0870 - val_accuracy: 0.8100\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0224 - accuracy: 0.9944 - val_loss: 1.1377 - val_accuracy: 0.8116\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0214 - accuracy: 0.9948 - val_loss: 1.2064 - val_accuracy: 0.8058\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0186 - accuracy: 0.9946 - val_loss: 1.2693 - val_accuracy: 0.8067\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 1.3595 - val_accuracy: 0.8074\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0159 - accuracy: 0.9957 - val_loss: 1.4066 - val_accuracy: 0.8065\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 1.4561 - val_accuracy: 0.8030\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0113 - accuracy: 0.9972 - val_loss: 1.5277 - val_accuracy: 0.8067\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0132 - accuracy: 0.9959 - val_loss: 1.6163 - val_accuracy: 0.7979\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 1.6426 - val_accuracy: 0.8031\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 1.6900 - val_accuracy: 0.8047\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 1.7742 - val_accuracy: 0.8015\n"
     ]
    }
   ],
   "source": [
    "path = \"logs2/fit\" + datetime.datetime.now().strftime(\"%Y%m%d - %H%M%S\")\n",
    "tensor_callback = tf.keras.callbacks.TensorBoard(log_dir = path,histogram_freq = 1)\n",
    "\n",
    "history = model.fit(training_padded,training_labels,epochs = num_epochs,\n",
    "                    validation_data = (testing_padded,testing_labels),callbacks = [tensor_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "nominated-plaza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "corrected-smart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14952), started 1:03:22 ago. (Use '!kill 14952' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f0c1dc0c5fa3f65f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f0c1dc0c5fa3f65f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs2/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "heated-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89740384]]\n"
     ]
    }
   ],
   "source": [
    "sentence = ['granny starting to fear spiders in the garden might be real',]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences,maxlen = max_length,padding = padding_type,truncating = 'post')\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-lambda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:el-dorado] *",
   "language": "python",
   "name": "conda-env-el-dorado-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
